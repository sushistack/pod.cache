---
type: ai_engineer_core
number: 25
---

## 2025-12-30 트랜스포머 - GPT 시대의 서막을 연 핵심 기술 아키텍처

[📄 원본 파일 보기](raw/025.%20Day%204%20-%20Understanding%20Transformers%20The%20Architecture%20Behind%20GPT%20and%20LLMs.md)


<details>
<summary>트랜스포머: GPT 시대의 서막을 연 핵심 기술 아키텍처</summary>

### GPT, 그 이름에 담긴 의미
- GPT는 `Generative Pre-trained Transformer`의 약자입니다.
- Transformer(트랜스포머): GPT의 기반이 되는 신경망 `아키텍처(Architecture)`입니다.


### Attention Is All You Need
2017년 Google의 연구자들이 발표한 획기적인 논문인 `"Attention Is All You Need"`에서 시작됩니다.
당시의 주류 신경망 모델(RNN, LSTM)은 시퀀스 데이터 처리에는 강했지만, 장거리 의존성(long-range dependency) 문제와 병렬 처리의 어려움이 있었습니다.


#### 신경망의 발전 과정

* 전통적인 데이터 과학 모델: 통계 모델 기반으로, 특정 파라미터를 통해 결과를 예측합니다. 예를 들어, 개인의 여러 요소를 바탕으로 신용 점수를 예측하는 방식입니다.
* 신경망(Neural Network): 1950년대에 인간 뇌의 뉴런 연결 방식에 영감을 받아 고안되었습니다. 여러 '인공 뉴런'들이 연결되어 데이터 내의 복잡한 패턴을 학습하고 예측합니다.
* 심층 신경망(Deep Neural Network) / 딥러닝(Deep Learning): 신경망의 층(layer)을 깊게 쌓아 올려 더 복잡하고 추상적인 패턴을 학습할 수 있도록 발전한 형태입니다. '깊이'가 깊어질수록 모델의 학습 능력이 향상됩니다.

트랜스포머는 이러한 심층 신경망의 새로운 '아키텍처'를 제시했습니다.

```
핵심 아이디어: "입력 시퀀스에서 '무엇'에 집중할 것인가?"
이것이 트랜스포머의 'Self-Attention' 메커니즘의 본질입니다.
```

어텐션 메커니즘 덕분에 트랜스포머는 이전 모델보다 훨씬 더 큰 데이터셋과 더 깊은 신경망을 효율적으로 학습할 수 있게 되었고, 이는 곧 엄청난 확장성(Scalability)으로 이어졌습니다.


#### 트랜스포머는 왜 강력한가?

- 효율적인 최적화: 대규모 데이터셋과 방대한 `파라미터(Parameter)`를 훨씬 더 효율적이고 빠르게 학습할 수 있도록 돕는 `최적화된 접근 방식`입니다.
- 병렬 처리 능력: 어텐션 메커니즘은 시퀀스 내 모든 토큰을 동시에 처리할 수 있어, 순차적으로 처리해야 했던 기존 RNN 계열 모델의 한계를 극복하고 학습 시간을 크게 단축시켰습니다.
- 컨텍스트 윈도우(Context Window): 트랜스포머 기반 모델은 입력으로 주어지는 토큰들의 시퀀스 길이를 나타내는 `컨텍스트 윈도우`가 넓어, 더 많은 정보를 한 번에 고려하여 더 일관성 있고 맥락에 맞는 응답을 생성할 수 있습니다.

물론 트랜스포머만이 유일한 해결책은 아닙니다. `State Space Architectures`나 `하이브리드 아키텍처`와 같은 다른 대안들도 연구되고 있습니다.

</details>
