---
type: ai_engineer_core
number: 27
---

## 2025-12-31 νλΌλ―Έν„°μ™€ λ‘ κ°€μ§€ μ¤μΌ€μΌλ§ μ „λµ

[π“„ μ›λ³Έ νμΌ λ³΄κΈ°](raw/027.%20Day%204%20-%20Parameters%20From%20Millions%20to%20Trillions%20in%20GPT%2C%20LLaMA%20%26%20DeepSeek.md)


<details>
<summary>νλΌλ―Έν„°μ μ§„ν™”μ™€ μ¤μΌ€μΌλ§ μ „λµ</summary>

### νλΌλ―Έν„°, LLM μ§€λ¥μ κ·Όκ°„

νλΌλ―Έν„°λ” μ‹ κ²½λ§ λ¨λΈμ΄ ν•™μµ λ°μ΄ν„°λ¥Ό ν†µν•΄ μ–»μ€ μ§€μ‹μ„ μ €μ¥ν•λ” μλ§μ€ κ°€μ¤‘μΉ(weights)μ™€ νΈν–¥(biases) κ°’μ μ΄ν•©
νλΌλ―Έν„°μ μκ°€ λ§λ‹¤λ” κ²ƒμ€ λ¨λΈμ΄ λ” λ³µμ΅ν• ν¨ν„΄μ„ ν•™μµν•κ³  λ” λ§μ€ μ •λ³΄λ¥Ό μ €μ¥ν•  μ μλ” μ μ¬λ ¥μ„ κ°€μ§„λ‹¤λ” μλ―Έ

- GPT-1: 1μ–µ 1,700λ§ κ°
- GPT-2: 15μ–µ κ°
- GPT-3: 1,750μ–µ κ°
- GPT-4: 1μ΅° 7,600μ–µ κ° (μ¶”μ •)

### λ¨λΈ μ„±λ¥ ν–¥μƒμ„ μ„ν• λ‘ κ°€μ§€ μ¤μΌ€μΌλ§ μ „λµ

#### ν•™μµ μ‹μ  μ¤μΌ€μΌλ§ (Training Time Scaling)

νλΌλ―Έν„°κ°€ λ§μ„μλ΅ λ¨λΈμ€ λ” λ³µμ΅ν• ν¨ν„΄μ„ ν•™μµν•κ³  λ” λ°©λ€ν• μ •λ³΄λ¥Ό ν΅μν•  μ μμµλ‹λ‹¤. ν•μ§€λ§ μ΄λ” λ” λ§μ€ μ»΄ν“¨ν… μμ›κ³Ό μ‹κ°„μ΄ ν•„μ”ν•λ©°, ν›λ ¨ λΉ„μ©μ΄ κΈ°ν•κΈ‰μμ μΌλ΅ μ¦κ°€ν•©λ‹λ‹¤.

- Chinchilla μ¤μΌ€μΌλ§ λ²•μΉ™: λ‡ λ…„ μ „ μ μ•λ Chinchilla μ¤μΌ€μΌλ§ λ²•μΉ™μ€ λ¨λΈμ νλΌλ―Έν„° μμ™€ μ„±κ³µμ μΌλ΅ ν›λ ¨ν•  μ μλ” λ°μ΄ν„° μ–‘μ΄ λ€λµμ μΌλ΅ λΉ„λ΅€ν•λ‹¤λ” κ²ƒμ„ λ³΄μ—¬μ£Όμ—μµλ‹λ‹¤. μ¦‰, λ¨λΈμ΄ μ»¤μ§μλ΅ λ” λ§μ€ ν›λ ¨ λ°μ΄ν„°κ°€ ν•„μ”ν•λ‹¤λ” μλ―Έμ…λ‹λ‹¤.


#### μ¶”λ΅  μ‹μ  μ¤μΌ€μΌλ§ (Inference Time Scaling)

λ¨λΈ μμ²΄λ¥Ό μ¬ν›λ ¨ν•κ±°λ‚ ν™•μ¥ν•λ” λ€μ‹ , ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§, μ™Έλ¶€ μ •λ³΄ μ£Όμ… λ“±μ„ ν†µν•΄ λ¨λΈμ μ¶”λ΅  κ³Όμ •μ„ λ³΄μ΅°ν•μ—¬ λ” λ‚μ€ κ²°κ³Όλ¥Ό λ„μ¶ν•©λ‹λ‹¤.

- μ¶”λ΅  νΈλ¦­ (Reasoning Tricks): λ¨λΈμ—κ² λ‹¨μν λ‹µμ„ μƒμ„±ν•λ„λ΅ μ§€μ‹ν•λ” λ€μ‹ , 'λ‹¨κ³„λ³„λ΅ μƒκ°ν•λΌ'κ±°λ‚ 'λ‹µλ³€ λ°©μ‹μ„ λ¨Όμ € μ„¤λ…ν•λΌ'λ” λ“±μ μ§€μ‹λ¥Ό ν†µν•΄ μ‚¬κ³  κ³Όμ •μ„ μ λ„ν•λ” λ°©λ²•μ…λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ λ¨λΈμ€ λ” μ‹ μ¤‘ν•κ³  λ…Όλ¦¬μ μΈ λ‹µλ³€μ„ μƒμ„±ν•  μ μμµλ‹λ‹¤.

- μ…λ ¥ μ‹ν€€μ¤μ— μ •λ³΄ μ¶”κ°€ (Augmenting Input Sequence): λ¨λΈμ μ…λ ¥ ν”„λ΅¬ν”„νΈμ— λ” λ§μ€ κ΄€λ ¨ μ •λ³΄λ¥Ό ν¬ν•¨μ‹μΌ μ¶”λ΅  μ‹μ μ— λ¨λΈμ΄ μ°Έμ΅°ν•  μ μλ” λ§¥λ½μ„ ν’λ¶€ν•κ² λ§λ“λ” λ°©λ²•μ…λ‹λ‹¤.
    - RAG (Retrieval Augmented Generation): λ€ν‘μ μΈ μμ‹λ΅, μ™Έλ¶€ μ§€μ‹ λ² μ΄μ¤μ—μ„ κ΄€λ ¨ λ¬Έμ„λ¥Ό κ²€μƒ‰ν•μ—¬ λ¨λΈμ μ…λ ¥μΌλ΅ μ κ³µν•¨μΌλ΅μ¨ λ¨λΈμ΄ μµμ‹  μ •λ³΄λ¥Ό κΈ°λ°μΌλ΅ λ‹µλ³€μ„ μƒμ„±ν•λ„λ΅ λ•μµλ‹λ‹¤.


### μ¤μΌ€μΌλ§μ μ΅°ν™”: μ‹λ„μ§€ ν¨κ³Ό

μ΄λ¬ν• λ€κ·λ¨ λ¨λΈ μ¤‘ μƒλ‹Ήμλ” MoE (Mixture of Experts) μ•„ν‚¤ν…μ²λ¥Ό μ±„νƒν•κ³  μμµλ‹λ‹¤. MoE λ¨λΈμ€ μ—¬λ¬ κ°μ μ‘μ€ 'μ „λ¬Έκ°€' λ¨λΈλ“¤μ„ ν¬ν•¨ν•κ³  μμΌλ©°, μ£Όμ–΄μ§„ μ§λ¬Έμ— λ”°λΌ κ°€μ¥ μ ν•©ν• μ „λ¬Έκ°€ λ¨λΈλ“¤μ„ λ™μ μΌλ΅ ν™μ„±ν™”ν•μ—¬ μ‚¬μ©

=> μ΄λ” λ¨λ“  νλΌλ―Έν„°λ¥Ό ν•­μƒ ν™μ„±ν™”ν•λ” λ€μ‹  ν•„μ”ν• λ¶€λ¶„λ§ μ‚¬μ©ν•¨μΌλ΅μ¨, κ±°λ€ν• λ¨λΈμ„ ν¨μ¨μ μΌλ΅ μ΄μν•κ³  μ¶”λ΅  μ†λ„λ¥Ό ν–¥μƒμ‹ν‚¤λ” ν¨κ³Ό

</details>
