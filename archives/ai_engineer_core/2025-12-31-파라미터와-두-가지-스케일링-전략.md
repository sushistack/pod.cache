---
type: ai_engineer_core
number: 27
---

## 2025-12-31 파라미터와 두 가지 스케일링 전략

<details>
<summary>파라미터의 진화와 스케일링 전략</summary>

### 파라미터, LLM 지능의 근간

파라미터는 신경망 모델이 학습 데이터를 통해 얻은 지식을 저장하는 수많은 가중치(weights)와 편향(biases) 값의 총합
파라미터의 수가 많다는 것은 모델이 더 복잡한 패턴을 학습하고 더 많은 정보를 저장할 수 있는 잠재력을 가진다는 의미

- GPT-1: 1억 1,700만 개
- GPT-2: 15억 개
- GPT-3: 1,750억 개
- GPT-4: 1조 7,600억 개 (추정)

### 모델 성능 향상을 위한 두 가지 스케일링 전략

#### 학습 시점 스케일링 (Training Time Scaling)

파라미터가 많을수록 모델은 더 복잡한 패턴을 학습하고 더 방대한 정보를 흡수할 수 있습니다. 하지만 이는 더 많은 컴퓨팅 자원과 시간이 필요하며, 훈련 비용이 기하급수적으로 증가합니다.

- Chinchilla 스케일링 법칙: 몇 년 전 제안된 Chinchilla 스케일링 법칙은 모델의 파라미터 수와 성공적으로 훈련할 수 있는 데이터 양이 대략적으로 비례한다는 것을 보여주었습니다. 즉, 모델이 커질수록 더 많은 훈련 데이터가 필요하다는 의미입니다.


#### 추론 시점 스케일링 (Inference Time Scaling)

모델 자체를 재훈련하거나 확장하는 대신, 프롬프트 엔지니어링, 외부 정보 주입 등을 통해 모델의 추론 과정을 보조하여 더 나은 결과를 도출합니다.

- 추론 트릭 (Reasoning Tricks): 모델에게 단순히 답을 생성하도록 지시하는 대신, '단계별로 생각하라'거나 '답변 방식을 먼저 설명하라'는 등의 지시를 통해 사고 과정을 유도하는 방법입니다. 이를 통해 모델은 더 신중하고 논리적인 답변을 생성할 수 있습니다.

- 입력 시퀀스에 정보 추가 (Augmenting Input Sequence): 모델의 입력 프롬프트에 더 많은 관련 정보를 포함시켜 추론 시점에 모델이 참조할 수 있는 맥락을 풍부하게 만드는 방법입니다.
    - RAG (Retrieval Augmented Generation): 대표적인 예시로, 외부 지식 베이스에서 관련 문서를 검색하여 모델의 입력으로 제공함으로써 모델이 최신 정보를 기반으로 답변을 생성하도록 돕습니다.


### 스케일링의 조화: 시너지 효과

이러한 대규모 모델 중 상당수는 MoE (Mixture of Experts) 아키텍처를 채택하고 있습니다. MoE 모델은 여러 개의 작은 '전문가' 모델들을 포함하고 있으며, 주어진 질문에 따라 가장 적합한 전문가 모델들을 동적으로 활성화하여 사용

=> 이는 모든 파라미터를 항상 활성화하는 대신 필요한 부분만 사용함으로써, 거대한 모델을 효율적으로 운영하고 추론 속도를 향상시키는 효과

</details>
