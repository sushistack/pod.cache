# 030. 💡 LLM 딥다이브- 토큰화와 '기억'의 실체 파헤치기

대규모 언어 모델(LLM)의 작동 방식을 깊이 이해하는 것은 강력한 AI 애플리케이션 개발의 핵심입니다. 이 글에서는 텍스트가 어떻게 토큰으로 분해되고 모델에 의해 처리되는지, 그리고 LLM이 대화 맥락을 '기억'하는 것처럼 보이는 환상이 어떻게 구현되는지를 상세히 다룹니다. `tiktoken` 라이브러리를 사용한 실제 토큰화 과정을 통해 텍스트 처리의 기초를 다지고, LLM의 상태 비저장(stateless) 특성 및 대화 맥락 유지 전략을 코드와 함께 명확히 설명합니다.

## 서론: LLM 시대의 필수 지식, 텍스트 처리와 '기억' 관리

최근 대규모 언어 모델(LLM)은 놀라운 성능으로 우리의 일상과 업무 방식을 혁신하고 있습니다. 하지만 이러한 모델이 텍스트를 어떻게 이해하고, 복잡한 대화를 어떻게 이어나가는지에 대한 깊이 있는 이해 없이는 그 잠재력을 온전히 활용하기 어렵습니다. 특히, 텍스트를 모델이 처리할 수 있는 형태로 변환하는 '토큰화' 과정과, LLM이 마치 '기억'을 가지고 있는 것처럼 대화 맥락을 유지하는 '환상'의 원리는 LLM 개발자에게 필수적인 지식입니다.

본 아티클에서는 `tiktoken` 라이브러리를 활용하여 텍스트 토큰화의 기본 원리를 탐구하고, LLM의 근본적인 상태 비저장 특성을 명확히 설명합니다. 나아가, LLM이 대화 맥락을 '기억'하는 것처럼 보이도록 만드는 개발자의 전략을 코드 예제와 함께 상세히 분석함으로써, 보다 견고하고 효율적인 LLM 기반 애플리케이션을 구축하기 위한 기반 지식을 제공하고자 합니다.

## 텍스트의 해부학: `tiktoken`으로 이해하는 토큰화

대규모 언어 모델은 사람이 사용하는 자연어를 직접적으로 이해하지 못합니다. 대신, 텍스트를 더 작은 단위인 '토큰(Token)'으로 분해하고, 이 토큰들을 숫자로 인코딩하여 처리합니다. 이 과정이 바로 토큰화(Tokenization)입니다. `tiktoken`은 OpenAI 모델이 사용하는 토크나이저를 파이썬에서 편리하게 사용할 수 있도록 해주는 라이브러리입니다.

### 토큰화란 무엇인가?

토큰화는 텍스트를 의미 있는 단위로 나누는 과정입니다. 예를 들어, "안녕하세요"라는 문장은 하나의 토큰이 될 수도 있고, "안녕"과 "하세요"로 나뉠 수도 있습니다. 각 토큰에는 고유한 숫자 ID가 부여되며, LLM은 이 ID 시퀀스를 입력받아 다음 토큰을 예측하는 방식으로 작동합니다.

### `tiktoken`을 이용한 텍스트 인코딩 및 디코딩

`tiktoken` 라이브러리를 사용하여 텍스트를 토큰으로 인코딩하고, 다시 토큰 ID를 텍스트 조각으로 디코딩하는 과정을 살펴보겠습니다.

먼저, `tiktoken` 패키지를 설치합니다.
```bash
pip install tiktoken
```

다음 코드는 `gpt-4` 모델에 사용되는 토크나이저를 로드하고, 특정 텍스트를 인코딩 및 디코딩하는 예시입니다.

```python
import tiktoken

# GPT-4 모델에 사용되는 인코딩을 로드합니다.
# 실제 강의에서는 GPT-4.1 mini를 언급했으나, GPT-4 또는 gpt-3.5-turbo 등
# 일반적인 모델명을 사용하는 것이 더 보편적입니다.
encoding = tiktoken.encoding_for_model("gpt-4")

# 텍스트를 토큰 ID 리스트로 인코딩합니다.
text_to_encode = "Hi, my name is Ed."
token_ids = encoding.encode(text_to_encode)

print(f"원본 텍스트: '{text_to_encode}'")
print(f"인코딩된 토큰 ID: {token_ids}")

# 토큰 ID 리스트를 다시 텍스트로 디코딩합니다.
# 각 토큰 ID가 어떤 텍스트 조각을 나타내는지 확인합니다.
decoded_fragments = [encoding.decode_single_token_bytes(token_id).decode('utf-8') for token_id in token_ids]
print(f"디코딩된 텍스트 조각: {decoded_fragments}")
```

**실행 결과 예시:**
```
원본 텍스트: 'Hi, my name is Ed.'
인코딩된 토큰 ID: [12194, 11, 492, 2213, 308]
디코딩된 텍스트 조각: ['Hi', ',', ' my', ' name', ' is', ' Ed', '.']
```
*참고: 강의 자막 내용과 실제 `tiktoken` 버전 및 모델 인코딩에 따라 토큰 ID 및 디코딩 결과는 다를 수 있습니다. 위 예시는 일반적인 동작 방식을 보여줍니다.*

위 예시에서 볼 수 있듯이, `Hi, my name is Ed.`는 `Hi`, `,`, ` my`, ` name`, ` is`, ` Ed`, `.` 등의 텍스트 조각으로 나뉘고 각각 고유한 숫자 ID를 가집니다. 특히, 공백(` `)이 단어 앞에 붙어 토큰으로 처리되는 경우가 많다는 것을 알 수 있습니다.

### Subword 토큰화의 특징

흥미로운 예시로, "Banoffee"와 같은 단어는 하나의 토큰이 아닌 여러 개의 토큰으로 분할될 수 있습니다.

```python
text_to_encode_banoffee = "I like Banoffee pie."
token_ids_banoffee = encoding.encode(text_to_encode_banoffee)

print(f"원본 텍스트: '{text_to_encode_banoffee}'")
print(f"인코딩된 토큰 ID: {token_ids_banoffee}")
decoded_fragments_banoffee = [encoding.decode_single_token_bytes(token_id).decode('utf-8') for token_id in token_ids_banoffee]
print(f"디코딩된 텍스트 조각: {decoded_fragments_banoffee}")
```

**실행 결과 예시:**
```
원본 텍스트: 'I like Banoffee pie.'
인코딩된 토큰 ID: [40, 946, 26458, 25270, 784]
디코딩된 텍스트 조각: ['I', ' like', ' Ban', 'offee', ' pie', '.']
```
이처럼 "Banoffee"가 "Ban"과 "offee"로 나뉘는 현상은 'Subword 토큰화'의 대표적인 특징입니다. 이는 모델이 사전에 없는 단어나 복잡한 단어를 효율적으로 처리하기 위한 방법입니다. 자주 등장하는 단어는 하나의 토큰으로 처리되지만, 그렇지 않은 단어는 더 작은 하위 단위로 쪼개져서 처리됩니다.

### 토큰화의 활용

토큰화는 단순히 텍스트를 숫자로 바꾸는 것을 넘어 다양한 분야에서 활용됩니다.
*   **길이 측정:** LLM의 입력 길이는 토큰 단위로 제한됩니다. 토큰 수를 미리 계산하여 입력이 제한을 초과하지 않는지 확인할 수 있습니다.
*   **비용 예측:** LLM API 사용 비용은 주로 입력 및 출력 토큰 수에 따라 결정됩니다. 토큰 수를 알면 비용을 예측하고 최적화할 수 있습니다.
*   **모델 입력 준비:** 토큰화는 LLM에게 텍스트를 이해 가능한 형태로 제공하는 필수적인 전처리 과정입니다.

## LLM의 '기억': 환상과 현실

LLM과 대화할 때 우리는 마치 모델이 이전 대화 내용을 기억하고 맥락을 유지하는 것처럼 느낍니다. 하지만 이것은 놀라운 '환상'이며, LLM의 근본적인 작동 방식은 우리가 생각하는 '기억'과는 다릅니다.

### LLM은 기억력이 없다? 근본적인 상태 비저장(Stateless) 특성

LLM과의 모든 API 호출은 **완전히 독립적이고 상태가 없습니다(stateless)**. 즉, LLM 모델 자체는 이전에 받았던 요청이나 그에 대한 응답을 전혀 기억하지 못합니다. 매번 새로운 요청이 들어올 때마다 모델은 현재 입력만을 가지고 다음 토큰을 예측합니다.

다음 예시를 통해 LLM의 상태 비저장 특성을 확인해 보겠습니다. OpenAI API 클라이언트를 사용합니다.

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

# .env 파일에서 환경 변수 로드 (API 키 등)
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 첫 번째 대화: 이름 소개
messages_intro = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hi, I'm Ed."}
]

response_intro = client.chat.completions.create(
    model="gpt-4", # 또는 "gpt-3.5-turbo"
    messages=messages_intro
)

assistant_reply_intro = response_intro.choices[0].message.content
print(f"어시스턴트: {assistant_reply_intro}")
```

**실행 결과 예시:**
```
어시스턴트: Hi Ed! Nice to meet you. How can I assist you today?
```
LLM은 'Ed'라는 이름을 잘 인식하고 친근하게 응답했습니다. 이제 후속 질문을 던져보겠습니다.

### 기억의 환상: 대화 맥락 유지의 비밀

이제 어시스턴트에게 "내 이름이 뭐지?"라고 물어보겠습니다.

```python
# 후속 질문: 이름 다시 묻기 (이전 맥락 없이)
messages_followup_no_context = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's my name?"}
]

response_followup_no_context = client.chat.completions.create(
    model="gpt-4",
    messages=messages_followup_no_context
)

assistant_reply_followup_no_context = response_followup_no_context.choices[0].message.content
print(f"어시스턴트: {assistant_reply_followup_no_context}")
```

**실행 결과 예시:**
```
어시스턴트: I don't have access to your personal information, including your name. How can I assist you today?
```
놀랍게도, 어시스턴트는 방금 전에 "Hi Ed!"라고 인사했음에도 불구하고, "이름을 알 수 없다"고 답합니다. 이는 LLM이 이전 대화를 '기억'하지 못하며, 각 호출이 독립적이라는 명백한 증거입니다.

그렇다면 ChatGPT와 같은 서비스는 어떻게 이전 대화를 기억하는 것처럼 보이는 걸까요? 그 비밀은 바로 **"대화 기록 전체를 매번 새로운 요청으로 전달"**하는 트릭에 있습니다.

LLM이 대화 맥락을 유지하도록 하려면, 개발자가 직접 모든 이전 메시지를 포함하여 현재까지의 전체 대화 기록을 `messages` 리스트에 담아 매번 API 호출에 전달해야 합니다. 이때, `role` 필드는 `system`, `user` 외에 `assistant`를 사용하여 모델의 이전 응답도 함께 포함시켜야 합니다.

```python
# 전체 대화 기록을 포함하여 후속 질문
messages_full_context = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hi, I'm Ed."},
    {"role": "assistant", "content": assistant_reply_intro}, # LLM의 이전 응답 포함
    {"role": "user", "content": "What's my name?"}
]

response_full_context = client.chat.completions.create(
    model="gpt-4",
    messages=messages_full_context
)

assistant_reply_full_context = response_full_context.choices[0].message.content
print(f"어시스턴트: {assistant_reply_full_context}")
```

**실행 결과 예시:**
```
어시스턴트: Your name is Ed. How can I help you today, Ed?
```
이제 어시스턴트는 정확히 'Ed'가 당신의 이름임을 기억하고 응답합니다! 이는 LLM이 정말로 '기억'하는 것이 아니라, 우리가 이전 대화 기록 전체를 새로운 입력 시퀀스로 제공했기 때문에, 이 시퀀스 내에서 가장 가능성 있는 다음 토큰을 예측한 결과입니다.

**핵심 요약:**
1.  **모든 LLM 호출은 완전히 상태 비저장(stateless)입니다.**
2.  LLM은 이전 대화를 '기억'하지 못합니다.
3.  **대화 맥락을 유지하는 '환상'은 개발자가 전체 대화 기록을 매번 입력으로 전달함으로써 구현됩니다.**
4.  이 대화 기록에는 `system`, `user` 메시지뿐만 아니라 LLM의 이전 `assistant` 응답도 포함되어야 합니다.

### 비용과 성능 트레이드오프

전체 대화 기록을 매번 LLM에 전달하는 방식은 대화 맥락 유지에 필수적이지만, 중요한 고려 사항이 있습니다. 바로 **비용 증가**입니다.

*   **더 많은 입력 토큰:** 대화가 길어질수록 `messages` 리스트에 포함되는 토큰의 수가 증가합니다.
*   **더 많은 컴퓨팅 자원:** LLM은 입력으로 들어온 전체 토큰 시퀀스를 기반으로 다음 토큰을 예측해야 하므로, 토큰 수가 많아질수록 더 많은 컴퓨팅 자원을 소모합니다. 이는 곧 응답 시간 증가와 API 사용 비용 증가로 이어집니다.

비록 개별 토큰당 비용은 매우 저렴하지만, 긴 대화나 동시 요청이 많은 시나리오에서는 이러한 비용이 누적되어 상당한 부담이 될 수 있습니다. 따라서, 효율적인 LLM 애플리케이션 설계를 위해서는 대화 길이 관리, 요약 기법 도입, 캐싱 등 다양한 최적화 전략을 고려해야 합니다.

## 결론: LLM의 기본기를 다지는 여정

이 아티클을 통해 우리는 LLM 개발의 두 가지 핵심 개념인 텍스트 토큰화와 대화 맥락 유지의 비밀을 깊이 있게 탐구했습니다. `tiktoken`을 사용하여 텍스트가 어떻게 모델이 이해하는 토큰으로 변환되는지 실습하고, LLM의 근본적인 상태 비저장 특성에도 불구하고 마치 '기억'을 가지고 있는 것처럼 보이는 이유가 개발자의 정교한 전략 덕분임을 이해했습니다.

LLM은 마법이 아닙니다. 복잡한 계산과 영리한 설계 원리가 결합된 정교한 시스템입니다. 토큰화의 이해는 LLM의 입력 및 출력 구조를 파악하고 비용을 예측하는 데 필수적이며, LLM의 상태 비저장 특성과 대화 맥락 유지 전략에 대한 정확한 이해는 일관되고 유용한 대화형 AI를 구축하는 데 있어 초석이 됩니다.

이제 여러분은 LLM의 '기억'이 환상이라는 것을 알았고, 그 환상을 만드는 데 필요한 기술적 지식을 갖추었습니다. 이러한 기본 원리를 바탕으로 더욱 강력하고 사용자 친화적인 LLM 기반 애플리케이션을 개발할 수 있기를 바랍니다.