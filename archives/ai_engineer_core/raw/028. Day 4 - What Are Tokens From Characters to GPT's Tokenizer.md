# 028. 🚀 언어 모델의 핵심- 토큰과 토크나이저 이해하기

거대 언어 모델(LLM)이 텍스트를 이해하고 생성하는 방식의 근간에는 '토큰' 개념이 자리 잡고 있습니다. 본 아티클에서는 언어 모델이 텍스트를 처리하는 방식이 문자 단위, 단어 단위를 넘어 토큰 단위로 발전하게 된 배경과 토큰의 장점, 그리고 토크나이저의 역할에 대해 심도 있게 다룹니다. 토큰이 어떻게 LLM의 효율성과 성능 향상에 기여하는지 함께 살펴보겠습니다.

## 서론: 텍스트를 숫자로, 언어 모델의 첫걸음

최근 인공지능 분야에서 거대 언어 모델(LLM)의 활약은 눈부십니다. 이러한 모델들이 방대한 텍스트 데이터를 학습하고 새로운 문장을 생성하며 복잡한 질문에 답할 수 있는 능력은 어떻게 가능할까요? 그 핵심에는 컴퓨터가 이해할 수 있는 형태로 텍스트를 변환하는 과정, 즉 **토큰화(Tokenization)**가 있습니다. 언어 모델은 원시 텍스트를 직접 처리하는 것이 아니라, 이를 작은 단위인 '토큰(Token)'으로 분해한 뒤 각 토큰에 해당하는 숫자 ID를 입력으로 받습니다. 이 토큰의 개념은 모델의 성능과 효율성에 지대한 영향을 미치며, 그 발전 과정 자체가 AI 연구의 중요한 이정표가 되어왔습니다.

## 본론: 언어 모델의 텍스트 처리 단위, 그 진화의 역사

언어 모델이 텍스트를 처리하는 단위는 시대에 따라 효율성을 찾아 진화해왔습니다. 문자(Character) 단위부터 단어(Word) 단위, 그리고 오늘날의 토큰(Subword Unit) 단위에 이르기까지, 각 방식은 고유한 장단점을 가졌습니다.

### 텍스트 인코딩의 초기 시도: 문자 및 단어 단위

초기 신경망 모델은 주로 두 가지 방식으로 텍스트를 처리하려 했습니다.

#### 문자 단위(Character-level) 방식
*   **작동 방식:** 텍스트를 개별 문자로 분해하여 모델에 입력했습니다. 예를 들어, "hello"는 'h', 'e', 'l', 'l', 'o'로 나뉩니다.
*   **장점:**
    *   **작은 어휘 집합:** 대문자, 소문자, 구두점을 포함해도 약 100개 내외의 문자만 필요하므로, 어휘 집합(Vocabulary)의 크기가 매우 작습니다.
    *   **효율적인 메모리 사용:** 적은 메모리로도 처리가 가능했습니다.
*   **단점:**
    *   **의미 학습의 어려움:** 모델이 문자를 조합하여 단어를 만들고, 그 단어의 의미를 파악하는 것까지 동시에 학습해야 했습니다. 이는 모델에게 너무 많은 부담이었고, 언어의 복잡한 의미 구조를 이해하기 어려웠습니다.

#### 단어 단위(Word-level) 방식
*   **작동 방식:** 텍스트를 개별 단어로 분해하여 모델에 입력했습니다. 예를 들어, "hello world"는 'hello', 'world'로 나뉩니다.
*   **장점:**
    *   **직관적인 의미:** 각 단어가 명확한 의미 단위를 가지므로, 의미 학습에 유리할 것이라고 기대했습니다.
*   **단점:**
    *   **폭발적인 어휘 집합:** 언어의 풍부함과 고유 명사(장소, 인명 등)의 다양성 때문에 가능한 단어의 수가 엄청나게 많아졌습니다. 이는 어휘 집합의 크기를 비현실적으로 크게 만들었습니다.
    *   **희귀 단어 및 OOV(Out-Of-Vocabulary) 문제:** 모든 단어를 어휘 집합에 포함하기 어려워, 희귀 단어는 학습에서 제외되거나 "알 수 없음(UNKNOWN)" 토큰으로 처리되어 모델이 해당 단어를 이해하지 못하는 문제가 발생했습니다.

### 토큰: 문자 및 단어 방식의 '중간 지점'과 혁신

이러한 문제점을 해결하기 위해 등장한 것이 바로 **토큰(Token)** 기반의 접근 방식입니다. 토큰은 단어의 일부분, 완전한 단어, 또는 자주 함께 나타나는 두 단어 이상이 될 수 있는 유연한 텍스트 덩어리(chunk)를 의미합니다.

*   **토큰의 특징:**
    *   **유연한 단위:** "running"이라는 단어는 "run" + "##ing"처럼 어간과 접미사로 분리될 수 있습니다. 혹은 "New York"처럼 자주 쓰이는 두 단어가 하나의 토큰이 될 수도 있습니다.
    *   **제한된 어휘 집합:** 문자 단위처럼 어휘 집합이 너무 작아 의미 학습이 어려운 문제, 단어 단위처럼 어휘 집합이 너무 커지는 문제를 해결하며, 약 3만~10만 개 정도의 적절한 어휘 집합을 유지합니다.
    *   **문자 단위로의 회귀 가능:** 필요하다면 가장 작은 단위인 문자 단위까지 분해될 수 있으므로, 희귀 단어나 새로운 단어에 대해서도 완전히 OOV 문제를 겪지 않고 처리할 수 있습니다.
*   **토큰 방식의 장점:**
    *   **효율성:** 제한된 어휘 집합으로 모델 학습 및 추론 속도가 빠릅니다.
    *   **빠른 개념 학습:** 모델이 언어의 어간(stem)과 접미사(suffix) 같은 형태학적 특징을 효율적으로 인식하고 학습할 수 있도록 돕습니다. 예를 들어, "run", "running", "runs" 등은 모두 "run"이라는 공통된 어간을 공유하여 학습을 단순화합니다.
    *   **희귀 단어 처리:** 단어를 의미 있는 하위 단위(subword unit)로 분해함으로써, 어휘 집합에 없는 단어(OOV)라도 그 구성 요소를 통해 유추할 수 있게 합니다.

이러한 토큰은 현대 Transformer 아키텍처 기반의 언어 모델에서 핵심적인 입력 단위로 사용되며, 모델이 언어를 더욱 깊이 있고 효율적으로 이해하도록 돕습니다.

### 토큰과 벡터: 혼동하지 마세요!

일부 개발자들은 '토큰'과 '벡터' 개념을 혼동하기도 합니다. 하지만 이 둘은 명확히 다릅니다.

*   **토큰(Token):**
    *   텍스트를 분해한 가장 기본적인 입력 단위입니다.
    *   각 토큰은 고유한 **토큰 ID(Token ID)**라는 정수(integer)로 변환됩니다.
    *   이 토큰 ID가 언어 모델에 전달되는 **최초의 입력**입니다.

*   **벡터(Vector):**
    *   토큰 ID가 모델에 입력된 후, 임베딩 레이어(Embedding Layer)를 거쳐 변환되는 **수치 표현**입니다.
    *   각 토큰의 의미를 다차원 공간의 좌표로 표현한 것으로, 유사한 의미를 가진 토큰은 벡터 공간에서 가깝게 위치합니다.
    *   벡터는 신경망 내부에서 토큰 간의 관계와 의미를 계산하는 데 사용됩니다.

요약하자면, 토큰은 '무엇'을 입력할지에 대한 단위이고, 벡터는 '어떻게' 모델이 그 '무엇'을 연산할 수 있도록 변환할지에 대한 방식입니다.

```
# 간단한 토큰화 예시 (개념적)

text = "Hello, world! How are you?"

# 토크나이저를 통해 텍스트를 토큰 ID로 변환
# 실제 토크나이저는 훨씬 복잡한 알고리즘을 사용합니다.

tokenizer_output = {
    "tokens": ["Hello", ",", " world", "!", " How", " are", " you", "?"],
    "token_ids": [15496, 11, 2159, 0, 1025, 383, 345, 30]
}

print(f"텍스트: {text}")
print(f"토큰 리스트: {tokenizer_output['tokens']}")
print(f"토큰 ID 리스트: {tokenizer_output['token_ids']}")

# 이 토큰 ID 리스트가 LLM의 초기 입력이 됩니다.
```

### GPT 토크나이저 직접 체험하기

GPT 모델의 토크나이저가 텍스트를 어떻게 분해하는지 직접 확인해볼 수 있습니다. OpenAI는 이를 위한 웹 기반 도구를 제공합니다.

*   **OpenAI Tokenizer:** [platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)

이 도구에 텍스트를 입력하면 해당 텍스트가 어떤 토큰들로 분해되는지, 각 토큰이 몇 바이트를 차지하는지, 그리고 총 몇 개의 토큰으로 구성되는지 시각적으로 확인할 수 있습니다. 직접 텍스트를 입력하여 단어의 조각들이 어떻게 토큰화되는지 체험해보세요.

## 결론: 토큰, LLM 성능의 숨은 공신

토큰은 거대 언어 모델이 인간의 언어를 효율적으로 처리하고 깊이 있게 이해하는 데 필수적인 요소입니다. 문자 단위의 한계와 단어 단위의 비효율성을 극복한 토큰 방식은 적절한 어휘 집합 크기를 유지하면서도 언어의 형태학적 특징과 의미를 효과적으로 포착할 수 있게 했습니다.

토큰은 단순히 텍스트를 나누는 것을 넘어, 언어 모델의 학습 속도, 메모리 효율성, 그리고 가장 중요한 언어 이해 능력에 직접적인 영향을 미칩니다. `platform.openai.com/tokenizer`와 같은 도구를 통해 토큰화 과정을 직접 경험해보는 것은 LLM의 작동 원리를 이해하는 데 큰 도움이 될 것입니다. 앞으로 LLM 기술이 더욱 발전함에 따라 토크나이저와 토큰의 역할 또한 계속해서 진화할 것입니다. 이 핵심 개념을 명확히 이해하는 것은 AI 시대의 개발자들에게 중요한 자산이 될 것입니다.