---
type: ai_engineer_core
number: 28
---

## 2025-12-31 토큰과 토크나이저

<details>
<summary>텍스트 인코딩의 초기 시도와 토큰화</summary>

### 텍스트 인코딩의 초기 시도: 문자 및 단어 단위

1. 문자 단위(Character-level) 방식
    - 예를 들어, "hello"는 'h', 'e', 'l', 'l', 'o'로 나뉩니다.
    - 장점: 작은 어휘 집합: 대문자, 소문자, 구두점을 포함해도 약 100개 내외의 문자만 필요하므로, 어휘 집합(Vocabulary)의 크기가 매우 작습니다, 효율적인 메모리 사용: 적은 메모리로도 처리가 가능했습니다.
    - 단점: 단어의 의미를 정확하게 표현하기 어려움: 예를 들어, "hello"와 "h3llo"는 같은 의미를 가진 단어이지만, 이는 모델이 학습할 수 없습니다.

2. 단어 단위(Word-level) 방식
    - 예를 들어, "hello world"는 'hello', 'world'로 나뉩니다.
    - 장점: 단어의 의미를 정확하게 표현할 수 있습니다.
    - 단점: 어휘 집합(Vocabulary)의 크기가 매우 큽니다.

3. 토큰 단위(Tokn-level) 방식
    - "running"이라는 단어는 "run" + "##ing"처럼 어간과 접미사로 분리될 수 있습니다. 혹은 "New York"처럼 자주 쓰이는 두 단어가 하나의 토큰이 될 수도 있습니다.
    - 장점: 문자 단위처럼 어휘 집합이 너무 작아 의미 학습이 어려운 문제, 단어 단위처럼 어휘 집합이 너무 커지는 문제를 해결하며, 약 3만~10만 개 정도의 적절한 어휘 집합을 유지합니다.
    - 장점: 문자 단위로의 회귀 가능: 필요하다면 가장 작은 단위인 문자 단위까지 분해될 수 있으므로, 희귀 단어나 새로운 단어에 대해서도 완전히 OOV 문제를 겪지 않고 처리할 수 있습니다.
    - 단점: 토큰화 과정이 복잡하고 시간이 많이 소요됩니다, 토큰화 과정에서 발생하는 오버헤드(OVERHEAD)


### 토큰과 벡터

- 토큰(Token):
    - 텍스트를 분해한 가장 기본적인 입력 단위입니다.
    - 각 토큰은 고유한 **토큰 ID(Token ID)**라는 정수(integer)로 변환됩니다.
    - 이 토큰 ID가 언어 모델에 전달되는 최초의 입력입니다.

- 벡터(Vector):
    - 토큰 ID가 모델에 입력된 후, 임베딩 레이어(Embedding Layer)를 거쳐 변환되는 수치 표현입니다.
    - 각 토큰의 의미를 다차원 공간의 좌표로 표현한 것으로, 유사한 의미를 가진 토큰은 벡터 공간에서 가깝게 위치합니다.
    - 벡터는 신경망 내부에서 토큰 간의 관계와 의미를 계산하는 데 사용됩니다.


</details>
