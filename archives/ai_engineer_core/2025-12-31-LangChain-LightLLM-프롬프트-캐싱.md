---
type: ai_engineer_core
number: 41
---

## 2025-12-31 LangChain, LightLLM, 그리고 프롬프트 캐싱

<details>
<summary>LangChain, LightLLM, 프롬프트 캐싱</summary>

### 강력한 워크플로우를 위한 LangChain

다양한 모델, 데이터 소스, 메모리 관리 등 방대한 생태계를 지원하며, LLM 애플리케이션의 거의 모든 구성 요소를 추상화하여 제공

### LightLLM

가볍고 단순한 인터페이스"라는 본질에 충실한 프레임워크, 토큰 및 비용 추적이라는 LightLLM의 핵심 기능 제공

### 프롬프트 캐싱

- 프롬프트 캐싱은 동일하거나 매우 유사한 입력 컨텍스트를 여러 번 전송할 때, LLM 제공자가 이전에 처리했던 컨텍스트를 재활용하여 입력 토큰 비용을 절감하는 기능.
- 실제 테스트 결과, 캐싱이 적용될 경우 동일한 5만여 개의 입력 토큰에 대해 5배 이상의 비용 절감 효과.


- OpenAI:
    - "프롬프트의 시작 부분 일치": OpenAI의 캐싱은 프롬프트의 시작 부분이 이전 호출과 정확히 일치할 때 가장 효과적으로 작동합니다.
    - 동적 정보 배치: 현재 날짜나 시간처럼 자주 변하는 정보는 프롬프트의 맨 앞이 아닌 뒷부분에 배치해야 캐싱 효율을 극대화할 수 있습니다. 예를 들어, "오늘 날짜는 2023년 10월 27일입니다. 다음 햄릿 텍스트를 참고하여..." 보다는 "다음 햄릿 텍스트를 참고하여... 오늘 날짜는 2023년 10월 27일입니다."가 캐싱에 유리합니다.
    - 정적 컨텍스트 우선: 햄릿 전문과 같은 정적인 컨텍스트는 항상 프롬프트의 시작 부분에 배치하여, 여러 질문에 대해 이 부분이 지속적으로 캐시되도록 하는 것이 좋습니다.

- Anthropic:
    - 명시적 캐싱: Anthropic은 자동 캐싱을 지원하지 않으며, 개발자가 명시적으로 캐시를 "프라이밍(prime the cache)"해야 합니다.
    - 초기 비용 vs. 절감 효과: 캐시를 프라이밍하는 데는 약 25%의 추가 비용이 발생하지만, 일단 캐시된 후 재사용할 때는 10배 이상의 비용 절감 효과를 얻을 수 있어, 장기적으로 훨씬 큰 이득을 제공합니다.

- Google Gemini:
    - Gemini는 암묵적(implicit) 캐싱과 명시적(explicit) 캐싱 모드를 모두 지원합니다. 구체적인 작동 방식은 제공되는 문서를 참조하여 최적의 전략을 선택할 수 있습니다.

</details>
